{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "D44_01텍스트다루기.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_iGSrA0EqfYh",
        "ajqeQOUtr1oQ",
        "80NCRmZRscqo",
        "mWtc7b9MvCpJ",
        "faQ8f3iHutNv",
        "oz2qox0KvJNg",
        "R9tVp38Rv3tg",
        "IZuFq2_xwMwf",
        "3m9ywkqbxx0n",
        "fS4EMK31GNS_",
        "0G24ypewGNKX",
        "fH4BfXcvJQw_"
      ],
      "authorship_tag": "ABX9TyOMwVFr+ZONfuAmJlFRx5JX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boramkim0514/KOIPA_AI/blob/main/D44_01%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%8B%A4%EB%A3%A8%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xqsy_HPilmt"
      },
      "source": [
        "# Tokenization (토큰화) 이론\n",
        "\n",
        "어떤 텍스트에서 어디까지가 문장, 어디까지가 단어인지 나눠주는 과정\n",
        "    * 문장(Sentence Tokeninzation)\n",
        "    * 단어(Word Tokeniztion)\n",
        "    * 음절 (Subword Tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGRcFhC5jC18"
      },
      "source": [
        "sample_text=\"I never thought through love we'd be. Making one as lovely as she. But isn't she lovely made from love.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVhKKbh8jDZM"
      },
      "source": [
        "문장 토큰화\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHPpK-4wjHzz",
        "outputId": "fbda7697-8c18-44fd-852a-6caac6a11882"
      },
      "source": [
        "# 단순하게 온점을 이용해서 잘라내기\n",
        "tokenized_sentence = sample_text.split(\". \") #list 형태로 문장이 잘림\n",
        "tokenized_sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I never thought through love we'd be\",\n",
              " 'Making one as lovely as she',\n",
              " \"But isn't she lovely made from love.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUjPwzlzjUO7"
      },
      "source": [
        "단어 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmIvQW3CjZv1",
        "outputId": "2309c81d-4869-4584-b0f5-7fc183e7282c"
      },
      "source": [
        "tokenized_word = sample_text.split() #기본으로 공백 잘라짐\n",
        "tokenized_word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'never',\n",
              " 'thought',\n",
              " 'through',\n",
              " 'love',\n",
              " \"we'd\",\n",
              " 'be.',\n",
              " 'Making',\n",
              " 'one',\n",
              " 'as',\n",
              " 'lovely',\n",
              " 'as',\n",
              " 'she.',\n",
              " 'But',\n",
              " \"isn't\",\n",
              " 'she',\n",
              " 'lovely',\n",
              " 'made',\n",
              " 'from',\n",
              " 'love.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PExpbM0RjlTc"
      },
      "source": [
        "## 띄어쓰기(공백)로만 영어 문장 내 단어를 구분할 때의 문제점\n",
        "\n",
        "* We're Avengers!! : `[We're, Avengers!!]`\n",
        "* We are Avengers!! : `[We, are, Avengers!!]`\n",
        "* We are Avengers : `[We, are, Avengers]`\n",
        "\n",
        "단순하게 공백으로만 토큰화를 수행하면, 사람은 같은 문장이라는 것을 인지할 수 있으나, 하지만 기계는 위 세 문장이 다른 문장이라고 판단."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mSFJEbjoEFj"
      },
      "source": [
        "### 그럼 특수문자를 제거하면?\n",
        "* `[We, re, Avengers]`\n",
        "* `[We, are, Avengers]`\n",
        "* `[We, are, AVengers]`\n",
        "\n",
        "특수문자가 중요한 의미를 가지는 경우에도 특수문자를 제거하면?\n",
        "* $12.45 : `[12, 45]`\n",
        "* Mr. So : `[Mr, So]`\n",
        "* Mrs. Kim : `[Mrs, Kim]`\n",
        "* 192.168.0.1 : `[192, 168, 0, 1]`\n",
        "* Ph.D : `[Ph, D]`\n",
        "\n",
        "특수문자가 중요한 역할을 하는 경우에는 별로 효용적이지 못한 것 같다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laH4MqUroE2i"
      },
      "source": [
        "## 미리 준비된 영어단어 토크나이져 준비 하기\n",
        "* TreebankWordTokenizer 패키지\n",
        " * 영어 표준 토큰화 규격을 따라간다.\n",
        " * Penn Treebank Tokenization 규칙\n",
        "\n",
        "* TreebankWordTokenizer 규칙\n",
        " * 하이푼으로 구성된 단어는 하나의 단어로 유지\n",
        " * doesn't 같이 어포스트로피로 '접어'가 함께하는 단어는 따로 분리\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjyoU-gSoI2Y"
      },
      "source": [
        "# English Tokenization 실습\n",
        "설치 코드\n",
        "```\n",
        "Colab 및 Jupyter notebook에서 설치\n",
        "!pip install nltk\n",
        "\n",
        "일반 로컬 터미널에서 설치\n",
        "pip install nltk\n",
        "```\n",
        "JVM( Java Virtual Machine )이 설치 되어 있어야 한다! #GCC, Cmaker등을 이용해, Oracle Java를 쓰는데 오픈 jdk, 유무료차이"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4447toZUoNYi",
        "outputId": "c04a1b52-853f-4647-feca-15b43d04c435"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYla_hWao4Gd"
      },
      "source": [
        "sentence = \"Ain't nothin' sweeter, you want this sugar, don't ya?\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylP5wQcppGpx",
        "outputId": "a701a8bc-e16e-418d-834d-91ecef0bc160"
      },
      "source": [
        "print(sentence.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Ain't\", \"nothin'\", 'sweeter,', 'you', 'want', 'this', 'sugar,', \"don't\", 'ya?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOYlc9U4pVYy"
      },
      "source": [
        "# #nltk의 기본 Tokenizer 사용\n",
        "# from nlkt.tokenize import word_tokenize\n",
        "# print(word_tokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2jpsdRrqVer",
        "outputId": "68713736-cd02-4b4e-faa1-835f44b98934"
      },
      "source": [
        "# nltk의 기본 Tokenizer 사용\n",
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ai', \"n't\", 'nothin', \"'\", 'sweeter', ',', 'you', 'want', 'this', 'sugar', ',', 'do', \"n't\", 'ya', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iGSrA0EqfYh"
      },
      "source": [
        "#[English] WordPunctTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkkNofTgqriA",
        "outputId": "7a644bf8-60c3-458e-fc86-44a0d525bd58"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "print(tokenizer.tokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ain', \"'\", 't', 'nothin', \"'\", 'sweeter', ',', 'you', 'want', 'this', 'sugar', ',', 'don', \"'\", 't', 'ya', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlzJQ9keqzR4",
        "outputId": "1a0dc5f9-1a07-4c81-b908-c27999cffe37"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "print(tokenizer.tokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ai', \"n't\", 'nothin', \"'\", 'sweeter', ',', 'you', 'want', 'this', 'sugar', ',', 'do', \"n't\", 'ya', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiHGxqcYrnUi",
        "outputId": "3037084b-84dd-4ca6-e66d-a565af0625ac"
      },
      "source": [
        "print(tokenizer.tokenize(\"I'm Iron-Man\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', \"'m\", 'Iron-Man']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajqeQOUtr1oQ"
      },
      "source": [
        "# Korea Tokenization 실습\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80NCRmZRscqo"
      },
      "source": [
        "# English Tokenization 실습\n",
        "설치 코드\n",
        "```\n",
        "Colab 및 Jupyter notebook에서 설치\n",
        "!pip install konlpy\n",
        "\n",
        "일반 로컬 터미널에서 설치\n",
        "pip install konlpy\n",
        "```\n",
        "JVM( Java Virtual Machine )이 설치 되어 있어야 한다! #GCC, Cmaker등을 이용해, Oracle Java를 쓰는데 오픈 jdk, 유무료차이\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Znpvotfzsnd4",
        "outputId": "4a58bc0d-4305-432f-a06b-c8a14480079e"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4I_Iv-HspMw"
      },
      "source": [
        "## Twitter(Okt), 꼬꼬마(Kkma), 코모란(Komoran), 한나눔(Hannanum)\n",
        "from konlpy.tag import Hannanum, Kkma, Komoran, Okt\n",
        "\n",
        "hannanum = Hannanum()\n",
        "kkma = Kkma()\n",
        "komoran = Komoran()\n",
        "okt = Okt()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ-v05z0tWEU"
      },
      "source": [
        "sentence = \"좋으니 그 사람 솔직히 견디기 버거워\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZdQnzWLtro0"
      },
      "source": [
        "# konlpy 의 모든 형태소 분리기는 duck typing 기법을 활용하여\n",
        "# 명사 추출, 각 형태소별 토큰화, 형태소 토큰 및 종류를 튜플로 표시하는 기능이 통일\n",
        "\n",
        "def print_tokenizer(tokenizer, s):\n",
        "    print(tokenizer.nouns(s)) #명사만 추출\n",
        "    print(tokenizer.morphs(s)) # 각 형태소 별로 토큰화\n",
        "    print(tokenizer.pos(s)) #각 형태소 토큰 및 형태소 종류를 튜플로 표현"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o665-vUuQsS",
        "outputId": "1ce39560-72f3-4945-ec63-c092090de716"
      },
      "source": [
        "\n",
        "print_tokenizer(okt, sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['그', '사람']\n",
            "['좋으니', '그', '사람', '솔직히', '견디기', '버거워']\n",
            "[('좋으니', 'Adjective'), ('그', 'Noun'), ('사람', 'Noun'), ('솔직히', 'Adjective'), ('견디기', 'Verb'), ('버거워', 'Adjective')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWtc7b9MvCpJ"
      },
      "source": [
        "### 트위터 (Okt)**굵은 텍스트**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faQ8f3iHutNv"
      },
      "source": [
        "## Kkoma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwX26RMHvF7b",
        "outputId": "1c7f9a3d-d9fa-4a74-82bf-a863a92e9a8a"
      },
      "source": [
        "print_tokenizer(kkma, sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['사람']\n",
            "['좋', '으니', '그', '사람', '솔직히', '견디', '기', '버겁', '어']\n",
            "[('좋', 'VA'), ('으니', 'ECD'), ('그', 'MDT'), ('사람', 'NNG'), ('솔직히', 'MAG'), ('견디', 'VV'), ('기', 'ETN'), ('버겁', 'VA'), ('어', 'ECS')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz2qox0KvJNg"
      },
      "source": [
        "## 코모란 (Komoran) \n",
        "오탈자에 강건함\n",
        "okt 랑 kkoma 사이"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0psG7FCyv1wm",
        "outputId": "a9b94fc6-aa50-4bbc-f6ee-97590e6601b7"
      },
      "source": [
        "print_tokenizer(komoran, sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['사람']\n",
            "['좋', '으니', '그', '사람', '솔직히', '견디', '기', '버거워']\n",
            "[('좋', 'VA'), ('으니', 'EC'), ('그', 'MM'), ('사람', 'NNG'), ('솔직히', 'MAG'), ('견디', 'VV'), ('기', 'ETN'), ('버거워', 'NA')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9tVp38Rv3tg"
      },
      "source": [
        "### 한나눔( Hannanum )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZf8P9W8wKmv",
        "outputId": "d3a64f11-1878-4908-818f-ac31535e2d47"
      },
      "source": [
        "print_tokenizer(hannanum, sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['사람', '버거워']\n",
            "['좋', '으니', '그', '사람', '솔직히', '견디', '기', '버거워']\n",
            "[('좋', 'P'), ('으니', 'E'), ('그', 'M'), ('사람', 'N'), ('솔직히', 'M'), ('견디', 'P'), ('기', 'E'), ('버거워', 'N')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZuFq2_xwMwf"
      },
      "source": [
        "# Sentence Tokenization\n",
        "단순히 물음표, 느낌표, 온점(.)만으로 문장 잘라내면 비효율적 의미 퇴색"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j-H-GCuxCk2"
      },
      "source": [
        "[English] sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIRsyhMxxR5H"
      },
      "source": [
        "text = \"Since I'm actively looking for Ph.D. students. I get the same question a dozen times every year.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ5Z0A_AxSSP",
        "outputId": "7230dec0-ff5e-4d96-8a07-13324c9aea0e"
      },
      "source": [
        "# 온점을 이용해서 문장을 나누면 ph.D 의미없이 잘림\n",
        "print(text.split(\".\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Since I'm actively looking for Ph\", 'D', ' students', ' I get the same question a dozen times every year', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtmc-Gt1xYPW",
        "outputId": "e701b6fb-8700-458c-8eda-a97a81f96fbb"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Since I'm actively looking for Ph.D. students.\", 'I get the same question a dozen times every year.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSgVwS92xhUW",
        "outputId": "69da51f0-b202-4a22-cb8b-ebf210a50d5e"
      },
      "source": [
        "text = \"My IP address is 192.168.56.51. Hello World!\"\n",
        "print(text.split(\".\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['My IP address is 192', '168', '56', '51', ' Hello World!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i7ZcC_Oxqqh",
        "outputId": "8ddaa4a6-9d5d-40dd-89b8-8d5c9f1a9074"
      },
      "source": [
        "print(sent_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['My IP address is 192.168.56.51.', 'Hello World!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m9ywkqbxx0n"
      },
      "source": [
        "# [Korean] kss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q3Q7V3GCxaO",
        "outputId": "91f28e89-e1cd-44af-afed-eea318e85342"
      },
      "source": [
        "!pip install kss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kss in /usr/local/lib/python3.7/dist-packages (2.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JakVhFMpCy7A",
        "outputId": "5ad6f1d9-94c5-47f1-853f-a28df30a39e3"
      },
      "source": [
        "import kss\n",
        "text = \"제 아이피는 192.168.56.51 이에요. 자연어 처리가 재미있나요? ㅋㅋ 딥러닝 들어가면 뚝배기가 깨집니다.ㅋㅋ\"\n",
        "\n",
        "print(kss.split_sentences(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['제 아이피는 192.168.56.51 이에요.', '자연어 처리가 재미있나요? ㅋㅋ', '딥러닝 들어가면 뚝배기가 깨집니다.ㅋㅋ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkHHOQT1DDKI",
        "outputId": "ea7bec8a-8230-432a-f4b2-b0db81ff125d"
      },
      "source": [
        "## [Korean] 띄어쓰기 및 맞춤법 정리\n",
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
            "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to /tmp/pip-req-build-81q5z49y\n",
            "  Running command git clone -q https://github.com/haven-jeon/PyKoSpacing.git /tmp/pip-req-build-81q5z49y\n",
            "Requirement already satisfied (use --upgrade to upgrade): pykospacing==0.5 from git+https://github.com/haven-jeon/PyKoSpacing.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: tensorflow==2.5.0 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.5) (2.5.0)\n",
            "Requirement already satisfied: h5py==3.1.0 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.5) (3.1.0)\n",
            "Requirement already satisfied: argparse>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.5) (1.4.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.12.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (0.2.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.19.5)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (2.5.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (0.36.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (2.5.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (0.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.12)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.15.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (3.12.4)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.34.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (3.7.4.3)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py==3.1.0->pykospacing==0.5) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (57.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (1.31.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (4.5.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (3.1.1)\n",
            "Building wheels for collected packages: pykospacing\n",
            "  Building wheel for pykospacing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykospacing: filename=pykospacing-0.5-cp37-none-any.whl size=2255825 sha256=8ed67a6fb025715ba665b04b60ac2210d32e3a68da6b4cd38dcf76e7b7215372\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6hvoh4kt/wheels/4d/45/58/e26cb2b7f6a063d234158c6fd1e5700f6e15b99d67154340ba\n",
            "Successfully built pykospacing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj3u3SG3DaVx",
        "outputId": "d34a6950-a83a-485a-9cd1-f4b1289d9b5f"
      },
      "source": [
        "!pip install git+https://github.com/ssut/py-hanspell.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ssut/py-hanspell.git\n",
            "  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-rs2h1gxl\n",
            "  Running command git clone -q https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-rs2h1gxl\n",
            "Requirement already satisfied (use --upgrade to upgrade): py-hanspell==1.1 from git+https://github.com/ssut/py-hanspell.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from py-hanspell==1.1) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (3.0.4)\n",
            "Building wheels for collected packages: py-hanspell\n",
            "  Building wheel for py-hanspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-hanspell: filename=py_hanspell-1.1-cp37-none-any.whl size=4871 sha256=cc7b0d45453734edfae5808107abe11f3dfdd6968ce8d3d89d068de597574a1e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sdxhhxzo/wheels/0a/25/d1/e5e96476dbb1c318cc26c992dd493394fe42b0c204b3e65588\n",
            "Successfully built py-hanspell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXcCkUB1Dctg"
      },
      "source": [
        "from pykospacing import Spacing # 한국어 띄어쓰기 관리 패키지\n",
        "from hanspell import spell_checker # 한국어 맞춤법 관리"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcEpOesHDspq"
      },
      "source": [
        "spacing = Spacing() #띄어쓰기 관리 객체"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rChs5pBD6AS"
      },
      "source": [
        "text = \"4번놀고있지.4번은팀워크가없어.4번은개인주의야.4번은혼자밖에생각하지않아.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCuT935pEZ04",
        "outputId": "df44ff49-4b17-4e2f-d1dc-c65624f75c6d"
      },
      "source": [
        "spacing_text = spacing(text)\n",
        "print(spacing_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4번 놀고 있지.4번은 팀워크가 없어.4번은 개인주의야.4번은 혼자 밖에 생각하지 않아.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar9T7jK1Egbq",
        "outputId": "bea020e3-7352-4aa8-8e36-2bcd9175658e"
      },
      "source": [
        "hanspell_text = spell_checker.check(text)\n",
        "print(hanspell_text.checked)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4번 놀고 있지. 4번은 팀워크가 없어. 4번은 개인주의야. 4번은 혼자밖에 생각하지 않아.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMd2MWVtEle3",
        "outputId": "b5f22460-3b09-4315-90ad-415fbb94b50f"
      },
      "source": [
        "# 맞춤법 검사\n",
        "text = \"맞춤뻡 틀리면 외 않되?\"\n",
        "hanspell_text = spell_checker.check(text).checked\n",
        "print(hanspell_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "맞춤법 틀리면 왜 안돼?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS4EMK31GNS_"
      },
      "source": [
        "# 텍스트 정규화\n",
        "문장의 복잡도를 낮춰주는 과정. 복잡도가 낮아지기 때문에 **처리할 텍스트가 줄어든다**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G24ypewGNKX"
      },
      "source": [
        "## [English] 정규화 - Steamming 과정\n",
        "어간(stem)을 추출하는 과정 - 영어는 사전에 없는 이상한 단어가 나오는 경우가 있다. \n",
        "* Beautifful : beau(itful), beaut(y)\n",
        "* Allowance: allow\n",
        "* Medical: medic\n",
        "* Books : book \n",
        "* This : thi # This is apple 임을 인식할 필요가 없기 때문에..\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mst9NzafGfNQ"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-q8a7jyHgDp"
      },
      "source": [
        "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJEZOqKHHof3",
        "outputId": "10a80480-2572-47de-b6dd-42f909766b9f"
      },
      "source": [
        "words = word_tokenize(text)\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUGaEUHaHydm",
        "outputId": "f23c5dd0-b1b5-4632-a83c-30c344f9b0ab"
      },
      "source": [
        "stem_list = [porter_stemmer.stem(w) for w in words]\n",
        "print(stem_list) # for 문 컴프리헨션 기억하기!!! "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SPhAuXcIEim"
      },
      "source": [
        "* 언어의 단어 모양이 변경되는 형식은 **자연어 생성 모델**을 만들 떄는 사용하면x\n",
        "* 단순 분류나 회귀문제를 풀 때는 효과가 있을 수도 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff3obLjuIoYc",
        "outputId": "e09cefe9-4052-4260-9f07-6073f58637eb"
      },
      "source": [
        "words = [\"Serialize\", \"Allowance\", \"Allowed\", \"medical\", \"This\", \"Pretty\", \"Beautiful\"]\n",
        "print([porter_stemmer.stem(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['serial', 'allow', 'allow', 'medic', 'thi', 'pretti', 'beauti']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH4BfXcvJQw_"
      },
      "source": [
        "## [Korean] 정규화 - Okt사용(Stemming, Normalization)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ57ego-JqHg",
        "outputId": "6085ea4c-7d42-406d-8a3a-292b709f0c74"
      },
      "source": [
        "okt= Okt()\n",
        "text = \"이것도 모르고 저것도 모르고 아무것도 모르면 뭘 모르는지도 모르더라.\"\n",
        "\n",
        "print(okt.morphs(text)) #어간이 추출된 형태소 분리\n",
        "print(okt.morphs(text, stem=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['이', '것', '도', '모르고', '저', '것', '도', '모르고', '아무', '것', '도', '모르면', '뭘', '모르는지도', '모르더라', '.']\n",
            "['이', '것', '도', '모르다', '저', '것', '도', '모르다', '아무', '것', '도', '모르다', '뭘', '모르다', '모르다', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmQUpFCAJ28u",
        "outputId": "d0801ff4-c65c-49af-e126-138ee6fd8e2a"
      },
      "source": [
        "print(okt.pos(text))\n",
        "print(okt.pos(text, stem=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('모르고', 'Verb'), ('저', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('모르고', 'Verb'), ('아무', 'Modifier'), ('것', 'Noun'), ('도', 'Josa'), ('모르면', 'Verb'), ('뭘', 'Noun'), ('모르는지도', 'Verb'), ('모르더라', 'Verb'), ('.', 'Punctuation')]\n",
            "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('모르다', 'Verb'), ('저', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('모르다', 'Verb'), ('아무', 'Modifier'), ('것', 'Noun'), ('도', 'Josa'), ('모르다', 'Verb'), ('뭘', 'Noun'), ('모르다', 'Verb'), ('모르다', 'Verb'), ('.', 'Punctuation')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFQsM4ttKO81",
        "outputId": "3011ce1f-67a0-4c36-f157-382df854f353"
      },
      "source": [
        "text = \"딥러닝 진짱 어렵닼ㅋㅋㅋㅋ 이렇게 어려울지 몰랐엌ㅋㅋㅋㅋㅋㅋㅋ\"\n",
        "print(okt.pos(text))\n",
        "print(okt.pos(text, norm=True)) #정규화"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('딥', 'Noun'), ('러닝', 'Noun'), ('진', 'Noun'), ('짱', 'Suffix'), ('어렵닼', 'Noun'), ('ㅋㅋㅋㅋ', 'KoreanParticle'), ('이렇게', 'Adverb'), ('어려울지', 'Verb'), ('몰랐엌', 'Noun'), ('ㅋㅋㅋㅋㅋㅋㅋ', 'KoreanParticle')]\n",
            "[('딥', 'Noun'), ('러닝', 'Noun'), ('진', 'Noun'), ('짱', 'Suffix'), ('어렵다', 'Adjective'), ('ㅋㅋㅋ', 'KoreanParticle'), ('이렇게', 'Adverb'), ('어려울지', 'Verb'), ('몰랐어', 'Verb'), ('ㅋㅋㅋ', 'KoreanParticle')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCp82wtHKix7",
        "outputId": "f56d9c3a-6bde-4d8a-b866-4796e13633cc"
      },
      "source": [
        "# 어간 추출, 정규화를 동시에\n",
        "print(okt.pos(text, stem=True, norm=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('딥', 'Noun'), ('러닝', 'Noun'), ('진', 'Noun'), ('짱', 'Suffix'), ('어렵다', 'Adjective'), ('ㅋㅋㅋ', 'KoreanParticle'), ('이렇게', 'Adverb'), ('어리다', 'Verb'), ('모르다', 'Verb'), ('ㅋㅋㅋ', 'KoreanParticle')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LURmwDCLChG"
      },
      "source": [
        "## [Korean] 이모티콘이나 의미없이 반복되는 문자 정제\n",
        "* ㅋㅋ, ㅋㅋㅋㅋㅋ, ㅋㅋㅋㅋㅋㅋㅋ\n",
        "* ㅎㅎㅎㅎㅎ\n",
        "잘한다 ㅠㅠㅠㅠ : 긍정의 표현으로 많이 사용 -> 잘한다 ㅠ\n",
        "잘한다 ㅋㅋㅋㅋ \"긍정 또는 약한 부정으로도 사용될 거 같다 0 \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX4zLq74LNpv",
        "outputId": "a2ecb3e1-d79f-486f-f4d2-9c501cac30d4"
      },
      "source": [
        "!pip install soynlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting soynlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/50/6913dc52a86a6b189419e59f9eef1b8d599cffb6f44f7bb91854165fc603/soynlp-0.0.493-py3-none-any.whl (416kB)\n",
            "\r\u001b[K     |▉                               | 10kB 16.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20kB 19.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 30kB 10.2MB/s eta 0:00:01\r\u001b[K     |███▏                            | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |████                            | 51kB 5.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 61kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 71kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 81kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 92kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 102kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 112kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 122kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 133kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 143kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 153kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 163kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 174kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 184kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 194kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 204kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 215kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 225kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 235kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 245kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 256kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 266kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 276kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 286kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 296kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 307kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 317kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 327kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 337kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 348kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 358kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 368kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 378kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 389kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 399kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 409kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 419kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (0.22.2.post1)\n",
            "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (5.4.8)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.0.1)\n",
            "Installing collected packages: soynlp\n",
            "Successfully installed soynlp-0.0.493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5uor6mCLo2d",
        "outputId": "48f8c101-106d-4e9d-e0ea-15fe3d4af29b"
      },
      "source": [
        "from soynlp.normalizer import emoticon_normalize\n",
        "\n",
        "print(emoticon_normalize(\"앜ㅋㅋㅋㅋㅋ 딥러닝 존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ\", num_repeats=2))\n",
        "print(emoticon_normalize(\"앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 딥러닝 존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ\", num_repeats=2))\n",
        "print(emoticon_normalize(\"앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 딥러닝 존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ\", num_repeats=2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "아ㅋㅋ 딥러닝 존잼쓰ㅠㅠ\n",
            "아ㅋㅋ 딥러닝 존잼쓰ㅠㅠ\n",
            "아ㅋㅋ 딥러닝 존잼쓰ㅠㅠ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26oYQU5AMOpn",
        "outputId": "78753d0e-3024-4e94-c773-9a3128eed1ff"
      },
      "source": [
        "# 반복되는 문자를 정규화\n",
        "from soynlp.normalizer import repeat_normalize\n",
        "\n",
        "print(repeat_normalize(\"문을 쿵쿵쿵쿵쿵쿵 두드렸다\", num_repeats=2))\n",
        "print(repeat_normalize(\"문을 쿵쿵쿵쿵쿵 두드렸다\", num_repeats=2))\n",
        "print(repeat_normalize(\"문을 쿵쿵쿵쿵 두드렸다\", num_repeats=2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문을 쿵쿵 두드렸다\n",
            "문을 쿵쿵 두드렸다\n",
            "문을 쿵쿵 두드렸다\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFv3j-4mORqJ"
      },
      "source": [
        "# 텍스트 정제 (Cleaning)\n",
        "* 정규식을 이용한 정제\n",
        "  * 특수기호나 의미 없는 공백등을 정규식을 활용하여 제거\n",
        "* 불용어(stopwords) 정제\n",
        "  * 빈도수가 낮거나, 짧거나, 의미 없는 단어를 문장에서 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p5Zxd13OM7P"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmolQuHhQhRI"
      },
      "source": [
        "eng_sent = \"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q51a_HqQ28-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7bab515-62ed-4a5e-a37c-b1fd7a3a89e2"
      },
      "source": [
        "# 위 문장에서 영어가 아닌 것들을 전부다 공백으로 치환\n",
        "# sub : replace와 같은 영할\n",
        "eng_sent = re.sub(\"[^a-zA-Z]\", \" \", eng_sent)\n",
        "print(eng_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Yeah  do you expect people to read the FAQ  etc  and actually accept hard atheism   No  you need a little leap of faith  Jimmy   Your logic runs out of steam         Jim   Sorry I can t pity you  Jim   And I m sorry that you have these feelings of denial about the faith you need to get by   Oh well  just pretend that it will all end happily ever after anyway   Maybe if you start a new newsgroup  alt atheist hard  you won t be bummin  so much        Bye Bye  Big Jim   Don t forget your Flintstone s Chewables          Bake Timmons  III\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFFl1NKtRhYm",
        "outputId": "0336afd8-84e5-493f-a182-734f5867d891"
      },
      "source": [
        "# 4글자 이상인 단어만 추출해서 문장을 새롭게 재구성하기\n",
        "eng_sent = \" \".join([ w for w in eng_sent.split() if len(w) > 3])\n",
        "print(eng_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Yeah expect people read actually accept hard atheism need little leap faith Jimmy Your logic runs steam Sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway Maybe start newsgroup atheist hard bummin much forget your Flintstone Chewables Bake Timmons\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFdlmfsDRpgG"
      },
      "source": [
        "#한국어 정규 표현식 정제\n",
        "\n",
        "한국어 문장에서 한글만 추출하는 정규식 표현: [ㄱ-ㅎㅏ-ㅣ가-힣]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNY2K7eHSbq7"
      },
      "source": [
        "kor_sent = \"느그 서장 남천동 살제?? 내가 임마 느그 서장이랑 으이??? hello world\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "w33vzp09Sy1T",
        "outputId": "62d01a46-17fc-4c83-8b35-2503dec5c8b0"
      },
      "source": [
        "kor_sent = re.sub(\"[^ㄱ-하-ㅣ가-힣]\", \" \", kor_sent)\n",
        "kor_sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'느그 서장 남천동 살제   내가 임마 느그 서장이랑 으이               '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pQKmW8QoS9Ei",
        "outputId": "8f9351a6-446a-4823-e5aa-05c439ce57b7"
      },
      "source": [
        "# 공백이 2개 이상이면 사라지게 하기. 공백 2개 이상을 1개로 치환하기\n",
        "kor_sent = re.sub(\"[ ]{2,}\", \" \", kor_sent) #공백 한글자가 / {} 반복 정규식 두번 이상 반복되면 하나로 묶어주겠다\n",
        "kor_sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'느그 서장 남천동 살제 내가 임마 느그 서장이랑 으이 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RBWZ31TMUNiK",
        "outputId": "f0df376d-d069-4c51-89bf-d9cc5bceecca"
      },
      "source": [
        "kor_sent.strip()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'느그 서장 남천동 살제 내가 임마 느그 서장이랑 으이'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSsSMw6eTZRr"
      },
      "source": [
        "# 불용어(stopwords) 정제\n",
        "필요 없는 짧은 단어나 의미가 없는 단어들을 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7LrGt8jUPSS",
        "outputId": "adf7c6b4-17c8-4b02-9463-effcdeec0eaa"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\") #불용어 사전 다운로드"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFd7C42uUyNr",
        "outputId": "213400ac-0b64-4a2f-d592-a7a912133013"
      },
      "source": [
        "# 불용어도 여러분들의 비즈니스에 맞게 설정할 수 있어야 한다.\n",
        "from nltk.corpus import stopwords\n",
        "list(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY6n4SW9U08J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_o6BN0mVDIy",
        "outputId": "77eab382-99ba-49ca-8f96-59e02fe329b4"
      },
      "source": [
        "example = \"Family is not an important thing. It's everything\"\n",
        "\n",
        "#불용어 사전중에 중복된 것이 있을 수도 있기 때문에 중복을 제거\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "word_tokens = word_tokenize(example.lower()) #문장을 소문자로 만든 다음 토큰화\n",
        "\n",
        "#불용어 사전에 들어있지 않은 단어라면 리스트에 추가\n",
        "result = [w for w in word_tokens if w not in stop_words]\n",
        "\n",
        "print(\"원본 : {}\".format(word_tokens))\n",
        "print(\"불용어 제거 후 : {}\".format(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "원본 : ['family', 'is', 'not', 'an', 'important', 'thing', '.', 'it', \"'s\", 'everything']\n",
            "불용어 제거 후 : ['family', 'important', 'thing', '.', \"'s\", 'everything']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTn4UHbiVcgc"
      },
      "source": [
        "# [Korean] 불용어 정제\n",
        "* 한국어 불용어 사전은 직접 만들거나 구글링해서 가져오면 된다.\n",
        "* 개발자가 직접 정의하는 것이 일반적\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2PDpYjIWEsx",
        "outputId": "682862b3-ec8b-4e49-a0dd-b27b386f8db4"
      },
      "source": [
        "example = \"내패와 정마담 패를 밑에서 뺏지. 내가 빙다리 핫바지로 보이냐?\"\n",
        "\n",
        "example_spell_check = spell_checker.check(example).checked\n",
        "\n",
        "word_tokens = okt.morphs(example_spell_check)\n",
        "\n",
        "print(word_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['대패', '와', '정마담', '패', '를', '밑', '에서', '뺏지', '.', '내', '가', '비', '에', '다리', '핫바', '지로', '보이냐', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN4E_wSBWTIS"
      },
      "source": [
        "# 개발자가 임의로 불용어를 선정할 수 있다.\n",
        "# 일반적으로 조사, 접속사들이 불용어로 선정된다.\n",
        "stop_words = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다', '것', '게']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjoRqTJuXCug",
        "outputId": "591fbad5-e222-483e-cccd-38d7f9ebc044"
      },
      "source": [
        "result = [w for w in word_tokens if not w in stop_words]\n",
        "\n",
        "print(\"원문 : \", word_tokens)\n",
        "print(\"불용어 제거 후 : {}\".format(result))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "원문 :  ['대패', '와', '정마담', '패', '를', '밑', '에서', '뺏지', '.', '내', '가', '비', '에', '다리', '핫바', '지로', '보이냐', '?']\n",
            "불용어 제거 후 : ['대패', '정마담', '패', '밑', '에서', '뺏지', '.', '내', '비', '다리', '핫바', '지로', '보이냐', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntD7h99BXHLA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}